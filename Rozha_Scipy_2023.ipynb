{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Rozha](https://github.com/ian-nai/Rozha) is a package to simplify and streamline a number of natural language processing processes and methods for a wide variety of languages, empowering users to use NLP on both non-English and English texts.\n",
        "\n",
        "Much of the work that has been done using natural language processing (NLP) has been focused on an Anglocentric model, using English texts in conjunction with tools and computer models that are primarily designed to work with the English language. Rozha was created to make it easier for people to begin engaging with non-English materials within the context of their NLP and digital humanities work. "
      ],
      "metadata": {
        "id": "2NvjSbHVV_wl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Started\n",
        "\n",
        "# Install using pip:\n",
        "\n",
        "!pip install rozha\n",
        "\n",
        "# Or download the GitHub repo and the install the requirements:\n",
        "\n",
        "# pip3 install -r requirements.txt\n",
        "\n",
        "# While using Colab, we need to set up our taggers in this way:\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "sgoT93cr6ZnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Then begin using the package by importing the modules you plan to use. Rozha is structured into three classes: process, analyze, and visualize. \n"
      ],
      "metadata": {
        "id": "01ha8N45XPYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you installed using pip, import them this way:\n",
        "\n",
        "import rozha.process as process # (or whatever name you choose)\n",
        "import rozha.analyze as analyze # (or whatever name you choose)\n",
        "import rozha.visualize as visualize # (or whatever name you choose)\n",
        "\n",
        "# If running from a local copy of the files, use the following:\n",
        "\n",
        "# from process import process\n",
        "# from analyze import analyze\n",
        "# from visualize import visualize"
      ],
      "metadata": {
        "id": "atIDaW68XTpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Example Pipelines\n",
        "\n",
        "The following pipelines show some examples of how to work with the package on non-English and multilingual texts. For a a full list of the package's functions, [follow this link](https://github.com/ian-nai/Rozha/blob/main/Functions.md).\n",
        "\n",
        "First, let's open a file, perform word tokenization and remove stopwords, make the text lowercase, and then get part-of-speech tags for the text:"
      ],
      "metadata": {
        "id": "JDHQ1T3MXEzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rozha.process as process\n",
        "import rozha.analyze as analyze\n",
        "\n",
        "word_tokenized = process.lowerFile(\"sample_text.txt\")\n",
        "pos_tags = analyze.posList(word_tokenized)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "id": "AUMtUtE_6eQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll perform sentence tokenization without removing stopwords, and then perform named entity recognition on each sentence using spaCy:"
      ],
      "metadata": {
        "id": "_yeG5KacpQwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rozha.process as process\n",
        "import rozha.analyze as analyze\n",
        "\n",
        "# Tokenizing our sentences and performing the named entity recognition\n",
        "sent_tokenized = process.allSentsVar(\"Jane runs fast. John runs slow.\", \"english\")\n",
        "ner_tags = analyze.spacyNer(sent_tokenized, \"en\")\n",
        "\n",
        "print(ner_tags)"
      ],
      "metadata": {
        "id": "tGX7haiMpVyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we'll perform word tokenization and remove stopwords from a string, make the text lowercase, and graph the 5 most common words as a bar graph:"
      ],
      "metadata": {
        "id": "NRQIR0qZpai7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rozha.process as process\n",
        "import rozha.visualize as visualize\n",
        "\n",
        "our_text = \"During the whole of a dull, dark, and soundless day in the autumn of the year, when the clouds hung oppressively low in the heavens, I had been passing alone, on horseback, through a singularly dreary tract of country; and at length found myself, as the shades of the evening drew on, within view of the melancholy House of Usher.\"\n",
        "no_stopwords = str(process.stopwordsVar(our_text, \"english\"))\n",
        "word_tokenized = process.lowerVar(no_stopwords)\n",
        "# Pass the var, number of words to graph, the height and width of the graph, and your preferred filename\n",
        "# The graph will save as \"my_graph.png\"\n",
        "visualize.barFreq(word_tokenized, 5, 400, 400, 'my_graph')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKc4zkpqpfOs",
        "outputId": "2c175ede-7172-4ccc-8af1-fe3bd97a83fc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['whole dull dark soundless day autumn year clouds hung oppressively low heavens passing alone horseback singularly dreary tract country length found shades evening drew within view melancholy house usher']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, let's detect the languages in a text and tag words according to which language they belong to."
      ],
      "metadata": {
        "id": "_02z2R0lpkeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rozha.process as process\n",
        "import rozha.analyze as analyze\n",
        "\n",
        "# Tokenizing into sentences\n",
        "sent_tokenized = process.sentTokenizeFile(\"sample_text.txt\")\n",
        "print(sent_tokenized)\n",
        "\n",
        "# # Now we detect the languages and print the output\n",
        "tagged_sents = analyze.detectLangVar(sent_tokenized)\n",
        "print(tagged_sents)"
      ],
      "metadata": {
        "id": "3Y36RoaTpsK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "The Rozha package ultimately aims to make multilingual digital humanities and natural language processing more accessible and to simplify the work of those already working in the field, and perhaps open up new avenues to explore for newcomers and established NLP practitioners. My hope is that this tool will help encourage diversity in the NLP landscape, and that people who may have felt it too daunting to work with materials in non-English languages may now feel more comfortable through the ease of working with this package.  Beyond that, I hope the package will serve as a conduit for additional contributions and collaboration, and that the code will ultimately help strengthen the field and community of practitioners working with non-English materials. "
      ],
      "metadata": {
        "id": "rLpNVVVdl0DV"
      }
    }
  ]
}
